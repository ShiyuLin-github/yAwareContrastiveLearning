{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6b2eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:01:56.219815200Z",
     "start_time": "2023-07-24T09:01:55.789614600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "My_transform = transforms.Compose([\n",
    "    # transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)), # 将维度重排为 [C, D, H, W]\n",
    "    #transforms.ToTensor(), # 将numpy数组转换为PyTorch张量\n",
    "    transforms.Normalize(mean=[0], std=[1]), # 归一化\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T09:01:58.659928200Z",
     "start_time": "2023-07-24T09:01:57.789138700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87918a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:02:01.476010600Z",
     "start_time": "2023-07-24T09:02:00.959109100Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_dir, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = os.listdir(root_dir)\n",
    "        self.csv_file = pd.read_csv(csv_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_path = os.path.join(self.root_dir, self.samples[idx])\n",
    "        \n",
    "        # Load each MRI sequence from the nii files,-6是除去文件夹名称里的_nifti\n",
    "        t1 = nib.load(os.path.join(sample_path, self.samples[idx][:-6] + '_T1.nii.gz')).get_fdata()\n",
    "        t1c = nib.load(os.path.join(sample_path, self.samples[idx][:-6] + '_T1c.nii.gz')).get_fdata()\n",
    "        t2 = nib.load(os.path.join(sample_path, self.samples[idx][:-6] + '_T2.nii.gz')).get_fdata()\n",
    "        flair = nib.load(os.path.join(sample_path, self.samples[idx][:-6] + '_FLAIR.nii.gz')).get_fdata()\n",
    "        asl = nib.load(os.path.join(sample_path, self.samples[idx][:-6] + '_ASL.nii.gz')).get_fdata()\n",
    "\n",
    "        # Combine the four MRI sequences into a single input tensor\n",
    "        # input_tensor = torch.Tensor(np.stack([t1, t1c, t2, flair, asl], axis=0))\n",
    "        input_tensor = torch.Tensor(t1)\n",
    "        # input_tensor = input_tensor.permute(0,3,1,2) #将维度重排为 [C, D, H, W]，有了transform就不需要在这里重排了，这个代码要求数据格式为[C, H, W, D]这里就不进行permute了\n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "        \n",
    "        #从csv文件中读取label\n",
    "        # 原ID比csv文件中的ID多了个0，比如UCSF-PDGM-0004，csv中是UCSF-PDGM-004。所以要修改一下\n",
    "        id = self.samples[idx][:-6]\n",
    "        id_fit = id[0:-4] + id[-3:]\n",
    "        label = self.csv_file.loc[self.csv_file['ID'] == id_fit, 'WHO CNS Grade'].values[0]\n",
    "        label = label - 2 #从[2,3,4]转为[0,1,2]\n",
    "        #PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss这里需要确保label是从0开始的,from深入浅出pytorch\n",
    "        \n",
    "        # Return the input tensor and any additional labels or targets\n",
    "        return input_tensor, label  # label是你的样本的标签，需要自己定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "#随机将数据集按比例分配为训练集和测试集\n",
    "custom_dataset = MyDataset(root_dir='I:/UCSF-PDGM-v3/PKG - UCSF-PDGM-v3/Mydeepmedic/UCSF-PDGM-v3',csv_dir='UCSF-PDGM-metadata_v2.csv')\n",
    "train_size = int(len(custom_dataset) * 0.7)\n",
    "test_size = len(custom_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T09:02:06.245622600Z",
     "start_time": "2023-07-24T09:02:02.584616700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 使用DataLoader加载CustomDataset以便逐批提取数据\n",
    "tri_dataloader = DataLoader(train_dataset, batch_size=4)#通过设置batchsize为整个dataset的len使得一个batch包含所有数据\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T09:02:11.538407900Z",
     "start_time": "2023-07-24T09:02:10.962747100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "#test\n",
    "data, labels = next(iter(tri_dataloader))\n",
    "# 将PyTorch张量转换为NumPy数组\n",
    "data_array = data.numpy()\n",
    "labels_array = labels.numpy()\n",
    "# 假设要保存的文件名分别为\"data.npy\"和\"labels.npy\"\n",
    "data_file_name = \"data.npy\"\n",
    "labels_file_name = \"labels.npy\"\n",
    "\n",
    "# 使用np.save函数将数据和标签保存为.npy文件\n",
    "np.save(data_file_name, data_array)\n",
    "np.save(labels_file_name, labels_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 240, 240, 155)\n",
      "[2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('data.npy')\n",
    "print(test_data.shape)\n",
    "test_labels = np.load('labels.npy')\n",
    "print(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T08:47:28.180183400Z",
     "start_time": "2023-07-24T08:47:28.053000400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#定义一个存取.npy文件的函数\n",
    "def SaveDataset(data_file_name,labels_file_name ,dataloader):\n",
    "    # 初始化空列表，用于保存所有数据\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    #提取训练集保存为.npy文件\n",
    "    # 遍历所有batch，并提取数据\n",
    "    for data, labels in dataloader:\n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # 将列表转换为NumPy数组\n",
    "    all_data_array = np.concatenate(all_data, axis=0)\n",
    "    all_labels_array = np.concatenate(all_labels, axis=0)\n",
    "    # 使用np.save函数将所有数据保存为.npy文件\n",
    "    np.save(data_file_name, all_data_array)\n",
    "    np.save(labels_file_name, all_labels_array)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T08:51:23.745669600Z",
     "start_time": "2023-07-24T08:51:23.735700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 68.1 MiB for an array with shape (240, 240, 155) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mSaveDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_data.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_labels.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[18], line 8\u001B[0m, in \u001B[0;36mSaveDataset\u001B[1;34m(data_file_name, labels_file_name, dataloader)\u001B[0m\n\u001B[0;32m      5\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#提取训练集保存为.npy文件\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 遍历所有batch，并提取数据\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data, labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m      9\u001B[0m     all_data\u001B[38;5;241m.\u001B[39mappend(data)\n\u001B[0;32m     10\u001B[0m     all_labels\u001B[38;5;241m.\u001B[39mappend(labels)\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[1;32mIn[22], line 19\u001B[0m, in \u001B[0;36mMyDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     17\u001B[0m t2 \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(sample_path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[idx][:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m6\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_T2.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mget_fdata()\n\u001B[0;32m     18\u001B[0m flair \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(sample_path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[idx][:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m6\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_FLAIR.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mget_fdata()\n\u001B[1;32m---> 19\u001B[0m asl \u001B[38;5;241m=\u001B[39m \u001B[43mnib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_ASL.nii.gz\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_fdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Combine the four MRI sequences into a single input tensor\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# input_tensor = torch.Tensor(np.stack([t1, t1c, t2, flair, asl], axis=0))\u001B[39;00m\n\u001B[0;32m     23\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(t1)\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\nibabel\\dataobj_images.py:373\u001B[0m, in \u001B[0;36mDataobjImage.get_fdata\u001B[1;34m(self, caching, dtype)\u001B[0m\n\u001B[0;32m    369\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache\n\u001B[0;32m    370\u001B[0m \u001B[38;5;66;03m# Always return requested data type\u001B[39;00m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001B[39;00m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;66;03m# during scaling\u001B[39;00m\n\u001B[1;32m--> 373\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masanyarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m caching \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfill\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fdata_cache \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\nibabel\\arrayproxy.py:439\u001B[0m, in \u001B[0;36mArrayProxy.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    419\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001B[39;00m\n\u001B[0;32m    420\u001B[0m \n\u001B[0;32m    421\u001B[0m \u001B[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;124;03m        Scaled image data with type `dtype`.\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 439\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_scaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    441\u001B[0m         arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\nibabel\\arrayproxy.py:406\u001B[0m, in \u001B[0;36mArrayProxy._get_scaled\u001B[1;34m(self, dtype, slicer)\u001B[0m\n\u001B[0;32m    404\u001B[0m     scl_inter \u001B[38;5;241m=\u001B[39m scl_inter\u001B[38;5;241m.\u001B[39mastype(use_dtype)\n\u001B[0;32m    405\u001B[0m \u001B[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001B[39;00m\n\u001B[1;32m--> 406\u001B[0m scaled \u001B[38;5;241m=\u001B[39m \u001B[43mapply_read_scaling\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_unscaled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslicer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslicer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscl_slope\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscl_inter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     scaled \u001B[38;5;241m=\u001B[39m scaled\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mpromote_types(scaled\u001B[38;5;241m.\u001B[39mdtype, dtype), copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Projects\\Github_Local\\yAwareContrastiveLearning\\venv\\lib\\site-packages\\nibabel\\volumeutils.py:925\u001B[0m, in \u001B[0;36mapply_read_scaling\u001B[1;34m(arr, slope, inter)\u001B[0m\n\u001B[0;32m    923\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr \u001B[38;5;241m*\u001B[39m slope1d\n\u001B[0;32m    924\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inter1d \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m--> 925\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[43marr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minter1d\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mreshape(shape)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 68.1 MiB for an array with shape (240, 240, 155) and data type float64"
     ]
    }
   ],
   "source": [
    "SaveDataset('val_data.npy','val_labels.npy',val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T09:03:07.530420100Z",
     "start_time": "2023-07-24T09:03:00.592533500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04399cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCSF-PDGM-352\n",
      "UCSF-PDGM-004\n",
      "UCSF-PDGM-007\n",
      "UCSF-PDGM-008\n",
      "torch.Size([4, 240, 240, 6])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create a dataloader for the dataset\n",
    "    train_dataset = MyDataset(root_dir='data/MRI/Train',csv_dir='data/UCSF-PDGM-metadata_v2.csv', transform = My_transform)\n",
    "    test_dataset = MyDataset(root_dir='data/MRI/Test',csv_dir='data/UCSF-PDGM-metadata_v2.csv', transform = My_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    samples, labels = next(iter(train_loader))\n",
    "    print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de822ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
